@InProceedings{emnlp15learn,
  title     = {An Empirical Analysis of Optimization for Max-Margin NLP},
  author    = {Jonathan K. Kummerfeld  and  Taylor Berg-Kirkpatrick  and  Dan Klein},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  shortvenue = {EMNLP},
  month     = {September},
  year      = {2015},
  location  = {Lisbon, Portugal},
  pages     = {273--279},
  url       = {http://www.jkk.name/pub/emnlp15learn.pdf},
  poster    = {http://www.jkk.name/pub/emnlp15learn_poster.png},
  software  = {https://github.com/tberg12/murphy},
  abstract  = {Despite the convexity of structured max-margin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and find several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal  optimization methods are often more robust and progress faster than dual methods. This advantage  is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives.},
}

@Article{astro13clouds,
  title     = {High-velocity Clouds in the Galactic All Sky Survey. I. Catalog},
  author    = {Vanessa A. Moss and Naomi M. McClure-Griffiths and Tara Murphy and D. J. Pisano and Jonathan K. Kummerfeld and James R. Curran},
  volume    = {209},
  number    = {1},
  pages     = {12},
  publisher = {IOP Publishing},
  journal   = {The Astrophysical Journal Supplement Series},
  year      = {2013},
  abstract  = {We present a catalogue of high-velocity clouds (HVCs) from the Galactic All Sky Survey (GASS) of southern-sky neutral hydrogen, which has 57 mK sensitivity and 1 km/s velocity resolution and was obtained with the Parkes Telescope. Our catalogue has been derived from the stray-radiation corrected second release of GASS. We describe the data and our method of identifying HVCs and analyse the overall properties of the GASS population. We catalogue a total of 1693 HVCs at declinations < 0 deg, including 1111 positive velocity HVCs and 582 negative velocity HVCs. Our catalogue also includes 295 anomalous velocity clouds (AVCs). The cloud line-widths of our HVC population have a median FWHM of ~19 km/s, which is lower than found in previous surveys. The completeness of our catalogue is above 95\% based on comparison with the HIPASS catalogue of HVCs, upon which we improve with an order of magnitude in spectral resolution. We find 758 new HVCs and AVCs with no HIPASS counterpart. The GASS catalogue will shed an unprecedented light on the distribution and kinematic structure of southern-sky HVCs, as well as delve further into the cloud populations that make up the anomalous velocity gas of the Milky Way.},
  url       = {http://iopscience.iop.org/0067-0049/209/1/12},
}

@InProceedings{emnlp13analysis,
  title     = {Error-Driven Analysis of Challenges in Coreference Resolution},
  author    = {Jonathan K. Kummerfeld and Dan Klein},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  shortvenue = {EMNLP},
  month     = {October},
  year      = {2013},
  location  = {Seattle, Washington, USA},
  pages     = {265--277},
  software  = {https://jkk.name/berkeley-coreference-analyser/},
  url       = {http://www.aclweb.org/anthology/D13-1027},
  slides    = {http://www.jkk.name/pub/emnlp13analysis_keynote.key},
  slidespdf = {http://www.jkk.name/pub/emnlp13analysis_slides.pdf},
  abstract  = {Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.},
}

@InProceedings{acl13analysis,
  title     = {An Empirical Examination of Challenges in Chinese Parsing},
  author    = {Jonathan K. Kummerfeld and Daniel Tse and James R. Curran and Dan Klein},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  shortvenue = {ACL (short)},
  location  = {Sofia, Bulgaria},
  pages     = {98--103},
  month     = {August},
  year      = {2013},
  software  = {https://jkk.name/berkeley-parser-analyser/},
  url       = {http://www.aclweb.org/anthology/P13-2018},
  slides    = {http://www.jkk.name/pub/acl13analysis_keynote.key},
  slidespdf = {http://www.jkk.name/pub/acl13analysis_slides.pdf},
  abstract  = {Aspects of Chinese syntax result in a distinctive mix of parsing challenges. However, the contribution of individual sources of error to overall difficulty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges.},
}

@InProceedings{emnlp12analysis,
  title     = {Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output},
  author    = {Jonathan K. Kummerfeld and David Hall and James R. Curran and Dan Klein},
  booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
  shortvenue = {EMNLP},
  year      = {2012},
  pages     = {1048--1059},
  month     = {July},
  location  = {Jeju Island, South Korea},
  software  = {https://jkk.name/berkeley-parser-analyser/},
  url       = {http://www.aclweb.org/anthology/D12-1096},
  slides    = {http://www.jkk.name/pub/emnlp12analysis_keynote.key},
  slidespdf = {http://www.jkk.name/pub/emnlp12analysis_slides.pdf},
  abstract  = {Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors.  We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.},
}

@InProceedings{acl12conversion,
  title     = {Robust Conversion of CCG Derivations to Phrase Structure Trees},
  author    = {Jonathan K. Kummerfeld and Dan Klein and James R. Curran},
  booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  shortvenue = {ACL (short)},
  year      = {2012},
  pages     = {105--109},
  month     = {July},
  location  = {Jeju Island, South Korea},
  software  = {https://jkk.name/berkeley-ccg2pst/},
  url       = {http://www.aclweb.org/anthology/P12-2021},
  slides    = {http://www.jkk.name/pub/acl12conversion_keynote.key},
  slidespdf = {http://www.jkk.name/pub/acl12conversion_slides.pdf},
  abstract  = {We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1\% vs. 39.6\%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.},
}

@InProceedings{conll11coreference,
  title     = {Mention Detection: Heuristics for the OntoNotes annotations},
  author    = {Jonathan K. Kummerfeld and Mohit Bansal and David Burkett and Dan Klein},
  shortvenue = {CoNLL Shared Task},
  booktitle = {Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task},
  year      = {2011},
  pages     = {102--106},
  month     = {June},
  location  = {Portland, Oregon, USA},
  url       = {http://www.aclweb.org/anthology/W11-1916},
  poster    = {http://www.jkk.name/pub/conll11coreference_poster.pdf},
  abstract  = {Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. Including exact matching mention detection in this shared task added a new and challenging dimension to the problem, particularly for our system, which previously used a very permissive detection method. We improved this aspect of the system by adding filters based on the annotation scheme for OntoNotes and analysis of system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3 , Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10.},
}

@Article{prl10chemistry,
  title     = {Spatiotemporal Hierarchy of Relaxation Events, Dynamical Heterogeneities, and Structural Reorganization in a Supercooled Liquid},
  author    = {Raphael Candelier and Asaph Widmer-Cooper and Jonathan K. Kummerfeld and Olivier Dauchot and Giulio Biroli and Peter Harrowell and David R. Reichman},
  journal   = {Physical Review Letters},
  volume    = {105},
  number    = {13},
  pages     = {135702},
  numpages  = {4},
  year      = {2010},
  month     = {September},
  doi       = {10.1103/PhysRevLett.105.135702},
  publisher = {American Physical Society},
  url       = {http://prl.aps.org/abstract/PRL/v105/i13/e135702},
  abstract  = {We identify the pattern of microscopic dynamical relaxation for a two-dimensional glass-forming liquid. On short time scales, bursts of irreversible particle motion, called cage jumps, aggregate into clusters. On larger time scales, clusters aggregate both spatially and temporally into avalanches. This propagation of mobility takes place along the soft regions of the systems, which have been identified by computing isoconfigurational Debye-Waller maps. Our results characterize the way in which dynamical heterogeneity evolves in moderately supercooled liquids and reveal that it is astonishingly similar to the one found for dense glassy granular media.},
}

@InProceedings{coling10morph,
  title     = {Morphological Analysis Can Improve a CCG Parser for English},
  author    = {Matthew Honnibal and Jonathan K. Kummerfeld and James R. Curran},
  booktitle = {Proceedings of the 23rd International Conference on Computational Linguistics},
  shortvenue = {CoLing},
  year      = {2010},
  pages     = {445--453},
  location  = {Beijing, China},
  month     = {August},
  url       = {http://aclweb.org/anthology/C/C10/C10-2051.pdf},
  abstract  = {Because English is a low morphology language, current statistical parsers tend to ignore morphology and accept some level of redundancy. This paper investigates how costly such redundancy is for a lexicalised grammar such as CCG.

We use morphological analysis to split verb inflectional suffixes into separate tokens, so that they can receive their own lexical categories. We find that this improves accuracy when the splits are based on correct POS tags, but that errors in gold standard or automatically assigned POS tags are costly for the system. This shows that the parser can benefit from morphological analysis, so long as the analysis is correct.},
}

@InProceedings{acl10adapt,
  title     = {Faster Parsing by Supertagger Adaptation},
  author    = {Jonathan K. Kummerfeld and Jessika Roesner and Tim Dawborn and James Haggerty and James R. Curran and Stephen Clark},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  shortvenue = {ACL},
  month     = {July},
  year      = {2010},
  location  = {Uppsala, Sweden},
  pages     = {345--355},
  software  = {http://downloads.schwa.org/acl10adapt_fast_news_model.tar.bz2},
  slidespdf = {http://www.jkk.name/pub/acl10adapt_slides.pdf},
  url       = {http://www.aclweb.org/anthology/P/P10/P10-1036.pdf},
  abstract  = {We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highest scoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text.},
}

@PhDThesis{thesis16parsing,
  title     = {Algorithms for Identifying Syntactic Errors and Parsing with Graph Structured Output},
  author    = {Jonathan K. Kummerfeld},
  year      = {2016},
  month     = {Aug},
  location  = {Berkeley, CA, USA},
  school    = {EECS Department, University of California, Berkeley},
  number    = {UCB/EECS-2016-138},
  url       = {https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-138.html},
  abstract  = {Representation of syntactic structure is a core area of research in Computational Linguistics, disambiguating distinctions in meaning that are crucial for correct interpretation of language. Development of algorithms and statistical models over the past three decades has led to systems that are accurate enough to be deployed in industry, playing a key role in products such as Google Search and Apple Siri. However, syntactic parsers today are usually constrained to tree representations of language, and performance is interpreted through a single metric that conveys no linguistic information regarding remaining errors.

  In this dissertation, we present new algorithms for error analysis and parsing. The heart of our approach to error analysis is the use of structural transformations to identify more meaningful classes of errors, and to enable comparisons across formalisms. For parsing, we combine a novel dynamic program with careful choices in syntactic representation to create an efficient parser that produces graph structured output. Together, these developments allowed us to evaluate the outstanding challenges in parsing and to address a key weakness in current work.

  First, we present a search algorithm that, given two structures, finds a sequence of modifications leading from one structure to the other. We applied this algorithm to syntactic error analysis, where one structure is the output of a parser, the other is the correct parse, and each modification corresponds to fixing one error. We constructed a tool based on the algorithm and analyzed variations in behavior between parsers, types of text, and languages. Our observations shine light on several assumptions about syntactic errors, showing some to be true and others to be false. For example, prepositional phrase attachment errors are indeed a major issue, while coordination scope errors do not hurt performance as much as expected.

  Next, we describe an algorithm that builds a parse in one syntactic representation to match a parse in another representation. Specifically, we build phrase structure parses from Combinatory Categorial Grammar derivations. Our approach follows the philosophy of CCG, defining specific phrase structures for each lexical category and generic rules for combinatory steps. The new parse is built by following the CCG derivation bottom-up, gradually building the corresponding phrase structure parse. This produced significantly more accurate parses than past work, and enabled us to compare performance of several parsers across formalisms.

  Finally, we address a weakness we observed in phrase structure parsers: the exclusion of syntactic trace structures for computational convenience. We present an efficient dynamic programming algorithm that constructs the graph structure that has the highest score under an edge-factored scoring function. We define a parse representation compatible with the algorithm, and show how certain linguistic distinctions dramatically impact coverage. We also show various ways to modify the algorithm to improve performance by exploiting properties of observed linguistic structure. This approach to syntactic parsing is the first to cover virtually all structure encoded in the Penn Treebank.},
}

@PhDThesis{thesis09adapt,
  title     = {Adaptive Supertagging for Faster Parsing},
  author    = {Jonathan K. Kummerfeld},
  school    = {The University of Sydney},
  year      = {2009},
  location  = {Sydney, Australia},
  url       = {http://www.jkk.name/pub/thesis09adapt_thesis.pdf},
  poster    = {http://www.jkk.name/pub/thesis09adapt_poster.pdf},
  slidespdf = {http://www.jkk.name/pub/thesis09adapt_slides.pdf},
  abstract  = {Statistical parsers are crucial for tackling the grand challenges of Natural Language Processing. The most effective approaches to these tasks are data driven, but parsers are too slow to be effectively used on large data sets. State-of-the-art parsers generally cannot process more than one sentence a second, and the fastest cannot process more than fifty sentences a second. The situation is even worse when they are applied outside of the domain of their training data. The fastest systems have two components, a parser, which has time complexity O(n3) and a supertagger, which has linear time complexity. By shifting work from the parser to the supertagger we dramatically improve speed.

  This work demonstrates several major novel ideas that improve parsing efficiency. The core idea is that the tags chosen by the parser are gold standard data for its supertagger. This leads to the second surprising conceptual development, that decreasing tagging accuracy can improve parsing performance. To demonstrate these ideas required extensive development of the C&C supertagger, including imple- mentation of more efficient estimation algorithms and parallelisation of the training process. This was particularly challenging as the C&C supertagger is a state-of-the-art high performance system designed with a focus on speed rather than flexibility.

  I was able to significantly improve performance on the standard evaluation corpus by using the parser to generate extremely large new resources for supertagger training. I have also shown that these methods provide significant benefits on another domain, Wikipedia text, without the cost of generating human annotated data sets. These parsing performance gains occur while supertagging accuracy decreases.

  Despite extensive use of supertaggers to improve parsing efficiency there has been no comprehensive study of the interaction between a supertagger and a parser. I present the first systematic exploration of the relationship, show the potential benefits of understanding it, and demonstrate a novel algorithm for optimising the parameters that define it.

  I have constructed models that process newspaper text 86\% faster than previously, and Wikipedia text 30\% faster, without any loss in accuracy and without the aid of extra gold standard resources in either domain. This work will lead directly to improvements in a range of Natural Language Processing tasks by enabling the use of far more parsed data.},
}

@InProceedings{alta09tagging,
  title     = {Faster parsing and supertagging model estimation},
  author    = {Jonathan K. Kummerfeld and Jessika Roesner and James R. Curran},
  booktitle = {Proceedings of the Australasian Language Technology Association Workshop 2009},
  shortvenue = {ALTA},
  year      = {2009},
  pages     = {62--70},
  location  = {Sydney, Australia},
  month     = {December},
  url       = {http://www.aclweb.org/anthology/U/U09/U09-1009.pdf},
  slidespdf = {http://www.jkk.name/pub/alta09tagging_slides.pdf},
  abstract  = {Parsers are often the bottleneck for data acquisition, processing text too slowly to be widely applied. One way to improve the efficiency of parsers is to construct more confident statistical models. More training data would enable the use of more sophisticated features and also provide more evidence for current features, but gold standard annotated data is limited and expensive to produce.

We demonstrate faster methods for training a supertagger using hundreds of millions of automatically annotated words, constructing statistical models that further constrain the number of derivations the parser must consider. By introducing new features and using an automatically annotated corpus we are able to double parsing speed on Wikipedia and the Wall Street Journal, and gain accuracy slightly when parsing Section 00 of the Wall Street Journal.},
}

@TechReport{report09jhu,
  title     = {Large-Scale Syntactic Processing: Parsing the Web},
  author    = {Stephen Clark and Ann Copestake and James R. Curran and Yue Zhang and Aurelie Herbelot and James Haggerty and Byung-Gyu Ahn and Curt Van Wyk and Jessika Roesner and Jonathan Kummerfeld and Tim Dawborn},
  year      = {2009},
  institution = {Johns Hopkins University},
  url       = {http://www.jkk.name/pub/report09jhu.pdf},
  abstract  = {Scalable syntactic processing will underpin the sophisticated language technology needed for next generation information access. Companies are already using nlp tools to create web-scale question answering and "semantic search" engines. Massive amounts of parsed web data will also allow the automatic creation of semantic knowledge resources on an unprecedented scale. The web is a challenging arena for syntactic parsing, because of its scale and variety of styles, genres, and domains.

The goals of our workshop were to scale and adapt an existing wide-coverage parser to Wikipedia text; improve the efficiency of the parser through various methods of chart pruning; use self-training to improve the efficiency and accuracy of the parser; use the parsed wiki data for an innovative form of bootstrapping to make the parser both more efficient and more accurate; and finally use the parsed web data for improved disambiguation of coordination structures, using a variety of syntactic and semantic knowledge sources.

The focus of the research was the C&C parser (Clark and Curran, 2007c), a state-of-the-art statistical parser based on Combinatory Categorial Grammar (ccg). The parser has been evaluated on a number of standard test sets achieving state-of-the-art accuracies. It has also recently been adapted successfully to the biomedical domain (Rimell and Clark, 2009). The parser is surprisingly efficient, given its detailed output, processing tens of sentences per second. For web-scale text processing, we aimed to make the parser an order of magnitude faster still. The C&C parser is one of only very few parsers currently available which has the potential to produce detailed, accurate analyses at the scale we were considering.},
}

@Article{chem08packing,
  title     = {The densest packing of AB binary hard-sphere homogeneous compounds across all size ratios},
  author    = {Jonathan K Kummerfeld and Toby S Hudson and Peter Harrowell},
  journal   = {The Journal of Physical Chemistry B},
  month     = {August},
  year      = {2008},
  volume    = {112},
  issue     = {35},
  pages     = {10773--10776},
  url       = {http://pubs.acs.org/doi/abs/10.1021/jp804953r},
  abstract  = {This paper considers the homogeneous packing of binary hard spheres in an equimolar stoichiometry, and postulates the densest packing at each sphere size ratio. Monte Carlo simulated annealing optimizations are seeded with all known atomic inorganic crystal structures, and the search is performed within the degrees of freedom associated with each homogeneous AB structure type. Structures isopointal to the FeB structure type are found to have the highest packing fraction at all sphere size ratios. The optimized structures match or improve on the best previously demonstrated packings of this type, and show that compound structures can pack more densely than segregated close-packed structures at all radius ratios less than 0.62.},
}

@InProceedings{alta08vpc,
  title     = {Classification of Verb Particle Constructions with the Google Web1T Corpus},
  author    = {Jonathan K. Kummerfeld and James R. Curran},
  booktitle = {Proceedings of the Australasian Language Technology Association Workshop 2008},
  shortvenue = {ALTA},
  month     = {December},
  year      = {2008},
  location  = {Hobart, Australia},
  pages     = {55--63},
  volume    = {6},
  url       = {http://www.aclweb.org/anthology/U/U08/U08-1008.pdf},
  poster    = {http://www.jkk.name/pub/alta08vpc_poster.eps},
  abstract  = {Manually maintaining comprehensive databases of multi-word expressions, for example Verb-Particle Constructions (VPCs), is infeasible. We describe a new classifier for potential VPCs, which uses information in the Google Web1T corpus to perform a simple linguistic constituency test. Specifically, we consider the fronting test, comparing the frequencies of the two possible orderings of the given verb and particle. Using only a small set of queries for each verb-particle pair, the system was able to achieve an F-score of 78.4\% in our evaluation while processing thousands of queries a second.},
}

@InProceedings{www17forums,
  title     = {Tools for Automated Analysis of Cybercriminal Markets},
  author    = {Rebecca S. Portnoff  and  Sadia Afroz  and  Greg Durrett  and  Jonathan K. Kummerfeld  and  Taylor Berg-Kirkpatrick  and  Damon McCoy  and  Kirill Levchenko  and  Vern Paxson},
  booktitle = {Proceedings of 26th International World Wide Web conference},
  shortvenue = {WWW},
  month     = {April},
  year      = {2017},
  location  = {Perth, Australia},
  url       = {http://www.jkk.name/pub/www17forums.pdf},
  software  = {http://evidencebasedsecurity.org/forums/},
  abstract  = {Underground forums are widely used by criminals to buy and sell a host of stolen items, datasets, resources, and criminal services.  These forums contain important resources for understanding cybercrime.  However, the number of forums, their size, and the domain expertise required to understand the markets makes manual exploration of these forums unscalable. In this work, we propose an automated, top-down approach for analyzing underground forums.  Our approach uses natural language processing and machine learning to automatically generate high-level information about underground forums, first identifying posts related to transactions, and then extracting products and prices. We also demonstrate, via a pair of case studies, how an analyst can use these automated approaches to investigate other categories of products and transactions. We use eight distinct forums to assess our tools: Antichat, Blackhat World, Carders, Darkode, Hack Forums, Hell, L33tCrew and Nulled. Our automated approach is fast and accurate, achieving over 80% accuracy in detecting post category, product, and prices.},
}

@InProceedings{acl17paraphrase,
  author    = {Youxuan Jiang  and  Jonathan K. Kummerfeld  and  Walter S. Lasecki},
  title     = {Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  shortvenue = {ACL (short)},
  month     = {July},
  year      = {2017},
  location  = {Vancouver, Canada},
  pages     = {103--109},
  abstract  = {Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.},
  url       = {http://aclweb.org/anthology/P17-2017},
  data      = {http://aclweb.org/anthology/attachments/P/P17/P17-2017.Datasets.zip},
  video     = {https://vimeo.com/234958413},
}

@InProceedings{tacl17parsing,
  author    = {Jonathan K. Kummerfeld  and  Dan Klein},
  title     = {Parsing with Traces: An O($n^4$) Algorithm and a Structural Representation},
  booktitle = {Transactions of the Association for Computational Linguistics},
  shortvenue = {TACL},
  year      = {2017},
  abstract  = {General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons.  We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference. In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena. We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner. Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3\% of the Penn English Treebank. We also implement a proofof-concept parser that recovers a range of null elements and trace types.},
  url       = {http://aclweb.org/anthology/Q17-1031},
  software  = {https://jkk.name/1ec-graph-parser/},
  interview = {https://soundcloud.com/nlp-highlights/46-parsing-with-traces-with-jonathan-kummerfeld},
  video     = {https://vimeo.com/238235203},
}

@InProceedings{emnlp17forums,
  author    = {Greg Durrett  and  Jonathan K. Kummerfeld  and  Taylor Berg-Kirkpatrick  and  Rebecca S. Portnoff  and  Sadia Afroz  and  Damon McCoy  and  Kirill Levchenko  and  and Vern Paxson},
  title     = {Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  shortvenue = {EMNLP},
  year      = {2017},
  month     = {September},
  location  = {Copenhagen, Denmark},
  url       = {https://www.aclweb.org/anthology/D17-1275},
  pages     = {2588--2597},
  abstract  = {One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects.  We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums.  Each of these forums constitutes its own "fine-grained domain" in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums.},
  software  = {https://evidencebasedsecurity.org/forums/},
}

@InProceedings{lrec18amr,
  author    = {Charles Welch  and  Jonathan K. Kummerfeld  and  Song Feng  and  Rada Mihalcea},
  title     = {World Knowledge for Abstract Meaning Representation Parsing},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  shortvenue = {LREC},
  year      = {2018},
  month     = {May},
  location  = {Miyazaki, Japan},
  url       = {http://www.lrec-conf.org/proceedings/lrec2018/pdf/1085.pdf},
  abstract  = {In this paper we explore the role played by world knowledge in semantic parsing. We look at the types of errors that currently exist in a state-of-the-art Abstract Meaning Representation (AMR) parser, and explore the problem of how to integrate world knowledge to reduce these errors. We look at three types of knowledge from (1) WordNet hypernyms and super senses, (2) Wikipedia entity links, and (3) retraining a named entity recognizer to identify concepts in AMR. The retrained entity recognizer is not perfect and cannot recognize all concepts in AMR and we examine the limitations of the named entity features using a set of oracles. The oracles show how performance increases if it can recognize different subsets of AMR concepts. These results show improvement on multiple fine-grained metrics, including a 6\% increase in named entity F-score, and provide insight into the potential of world knowledge for future work in Abstract Meaning Representation parsing.},
}

@InProceedings{naacl18data,
  author    = {Yiping Kang  and  Yunqi Zhang  and  Jonathan K. Kummerfeld  and  Parker Hill  and  Johann Hauswald  and  Michael A. Laurenzano  and  Lingjia Tang  and  Jason Mars},
  title     = {Data Collection for Dialogue System: A Startup Perspective},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)},
  shortvenue = {NAACL (industry)},
  pages     = {33--40},
  year      = {2018},
  month     = {June},
  location  = {New Orleans, Louisiana, USA},
  url       = {http://aclweb.org/anthology/N18-3005},
  abstract  = {Industrial dialogue systems such as Apple Siri and Google Assistant require large scale diverse training data to enable their sophisticated conversation capabilities. Crowdsourcing is a scalable and inexpensive data collection method, but collecting high quality data efficiently requires thoughtful orchestration of crowdsourcing jobs. Prior study of data collection process has focused on tasks with limited scope and performed intrinsic data analysis, which may not be indicative of impact on trained model performance. In this paper, we present a study of crowdsourcing methods for a user intent classification task in one of our deployed dialogue systems. Our task requires classification over 47 possible user intents and contains many intent pairs with subtle differences. We consider different crowdsourcing job types and job prompts, quantitatively analyzing the quality of collected data and downstream model performance on a test set of real user queries from production logs. Our observations provide insight into how design decisions impact crowdsourced data quality, with clear recommendations for future data collection for dialogue systems.},
  video     = {https://vimeo.com/277631102},
}

@InProceedings{naacl18summary,
  author    = {Youxuan Jiang  and  Catherine Finegan-Dollak  and  Jonathan K. Kummerfeld  and  Walter Lasecki},
  title     = {Effective Crowdsourcing for a New Type of Summarization Task},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  pages     = {628--633},
  shortvenue = {NAACL (short)},
  year      = {2018},
  month     = {June},
  location  = {New Orleans, Louisiana, USA},
  url       = {http://aclweb.org/anthology/N18-2099},
  abstract  = {Most summarization research focuses on summarizing the entire given text, but in practice readers are often interested in only one aspect of the document or conversation. We propose "targeted summarization" as an umbrella category for summarization tasks that intentionally consider only parts of the input data. This covers query-based summarization, update summarization, and a new task we propose where the goal is to summarize a particular aspect of a document. However, collecting data for this new task is hard because directly asking annotators (e.g., crowd workers) to write summaries leads to data with low accuracy when there are a large number of facts to include.  We introduce a novel crowdsourcing workflow, Pin-Refine, that allows us to collect highquality summaries for our task, a necessary step for the development of automatic systems.},
}

@InProceedings{naacl18embeddings,
  author    = {Laura Wendlandt  and  Jonathan K. Kummerfeld  and  Rada Mihalcea},
  title     = {Factors Influencing the Surprising Instability of Word Embeddings},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  shortvenue = {NAACL},
  year      = {2018},
  month     = {June},
  location  = {New Orleans, Louisiana, USA},
  url       = {http://aclweb.org/anthology/N18-1190},
  pages     = {2092--2102},
  abstract  = {Despite the recent popularity of word embedding methods, there is only a small body of work exploring the limitations of these representations. In this paper, we consider one aspect of embedding spaces, namely their stability. We show that even relatively high frequency words (100-200 occurrences) are often unstable. We provide empirical evidence for how various factors contribute to the stability of word embeddings, and we analyze the effects of stability on downstream tasks.},
}

@InProceedings{acl18sql,
  author    = {Catherine Finegan-Dollak\\*  and  Jonathan K. Kummerfeld\\*  and  Li Zhang  and  Karthik Ramanathan  and  Sesh Sadasivam  and  Rui Zhang  and  Dragomir Radev},
  title     = {Improving Text-to-SQL Evaluation Methodology},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  shortvenue = {ACL},
  month     = {July},
  year      = {2018},
  address   = {Melbourne, Victoria, Australia},
  pages     = {351--360},
  abstract  = {To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.},
  url       = {http://aclweb.org/anthology/P18-1033},
  software  = {https://jkk.name/text2sql-data},
  data      = {https://jkk.name/text2sql-data},
}

@Article{arxiv18disentangle,
  author    = {Jonathan K. Kummerfeld  and  Sai R. Gouravajhala  and  Joseph Peper  and  Vignesh Athreya  and  Chulaka Gunasekara  and  Jatin Ganhotra  and  Siva Sankalp Patel  and  Lazaros Polymenakos  and  Walter S. Lasecki},
  title     = {Analyzing Assumptions in Conversation Disentanglement Research Through the Lens of a New Dataset and Model},
  journal   = {ArXiv e-prints},
archivePrefix = {arXiv},
  eprint    = {1810.11118},
  primaryClass = {cs.CL},
  year      = {2018},
  month     = {October},
  abstract  = {Disentangling conversations mixed together in a single stream of messages is a difficult task with no large annotated datasets. We created a new dataset that is 25 times the size of any previous publicly available resource, has samples of conversation from 152 points in time across a decade, and is annotated with both threads and a within-thread reply-structure graph. We also developed a new neural network model, which extracts conversation threads substantially more accurately than prior work. Using our annotated data and our model we tested assumptions in prior work, revealing major issues in heuristically constructed resources, and identifying how small datasets have biased our understanding of multi-party multi-conversation chat.},
  url       = {https://arxiv.org/pdf/1810.11118.pdf},
}

